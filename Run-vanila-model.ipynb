{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miraj\\AppData\\Local\\Continuum\\anaconda3\\envs\\LM-FT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Define the model to infer\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "pipe = pipeline(\"text-generation\", model=model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Given the context , generate an SQL query for the user's question.\n",
      "Context:CREATE TABLE head(age INTEGER)</s>\n",
      "<|user|>\n",
      "How many heads of the departments are older than 56?</s>\n",
      "<|assistant|>\n",
      "Since the user did not specify the exact age range for each department, I am providing an example answer based on the given context.\n",
      "\n",
      "In the given context, the user is asking for the number of heads of the departments that are older than 56. The query to accomplish this task can be:\n",
      "\n",
      "```\n",
      "SELECT COUNT(*) FROM head WHERE age > 56;\n",
      "```\n",
      "\n",
      "In this query, the age column is used as the filter criteria. The count of rows returned by the query will give the number of heads of the departments that are older than 56.\n"
     ]
    }
   ],
   "source": [
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Given the context , generate an SQL query for the user's question.\\nContext:CREATE TABLE head(age INTEGER)\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many heads of the departments are older than 56?\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LM-FT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
