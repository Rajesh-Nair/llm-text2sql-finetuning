#ModelArguments 
model_name_or_path : "TinyLlama/TinyLlama-1.1B-Chat-v1.0" 
chat_template_format : "" 
lora_alpha : 16 
lora_dropout : 0.1 
lora_r : 8 
lora_target_modules : "all-linear" 
use_nested_quant : True 
bnb_4bit_compute_dtype : "bfloat16" 
bnb_4bit_quant_storage_dtype : "bfloat16" 
use_flash_attn : True 
use_peft_lora : True 
use_4bit_quantization : True 
use_reentrant : True 

#DataTrainingArguments 
dataset_name : "b-mc2/sql-create-context" 
packing : True 
dataset_text_field : "content" 
max_seq_length : 2048 
append_concat_token : False 
add_special_tokens : False 
splits : "train,test" 

# TrainingArguments 
seed : 100 
num_train_epochs : 4 
logging_steps : 5 
log_level : "info" 
logging_strategy : "steps" 
evaluation_strategy : "epoch" 
save_strategy : "epoch" 
bf16 : True 
learning_rate : 0.0001 
lr_scheduler_type : "cosine" 
weight_decay : 0.0001 
warmup_ratio : 0.0 
max_grad_norm : 1.0 
output_dir : "Tinyllama-ft-qlora-dsz3" 
per_device_train_batch_size : 4 
per_device_eval_batch_size : 4 
gradient_accumulation_steps : 2 
gradient_checkpointing : True 